## Ways to Evaluate Prompt Performance
```
Greetings, humans, and welcome back. Thanks for sticking with me to this next module in the course about getting started with prompt engineering. In this module, we move on to evaluate how the robots did. In other words, did our AI models give us good responses to the prompts that we passed in. Coming up, we'll cover objective metrics, subjective metrics, and then techniques for evaluating them, including surveys and interviews, as well as A/B testing. Now when you get into fine tuning with code using the APIs, there are some additional metrics that you can use to do these evaluations. I'll point you to some resources for that if you're interested, but for this getting started course, we're primarily going to focus on metrics that humans can look at and evaluate. And then we'll wrap up with an example of how to adjust parameters to get better results.
```

## Notes
- Module focuses on evaluating AI model responses to prompts in prompt engineering.
- Topics include objective and subjective metrics evaluation techniques.
    - Techniques comprise surveys, interviews, and A/B testing for assessment.
    - Mention of additional metrics for code fine-tuning using APIs.
    - Emphasis on human-readable metrics
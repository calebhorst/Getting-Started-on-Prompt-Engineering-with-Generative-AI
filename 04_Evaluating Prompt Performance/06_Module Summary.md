## Module Summary
```
We've covered quite a bit in this module, so let's take a quick minute to summarize the key takeaways before we move on. There are a variety of metrics that you can use to evaluate prompt performance. We saw objective metrics, things like correctness, speed, and relevancy and then some subjective metrics like coherence, tone, and so on. We also took a look at some evaluation techniques you can use like surveys, interviews, and A/B testing. As you're doing your testing, you can adjust different parameters to get different or better results. We saw the results of adjusting things like temperature, frequency penalty, and presence penalty to get different results from the model. And we also saw that really handy token highlighting that gives you some insight into what's happening behind the scenes with the model picking words based on the probability that that word comes next. And with that, let's move on. Join me in the next module where we'll cover advanced prompting techniques. I'll see you there.
```

## Notes
Absolutely! Here's a brief summary of the key takeaways:

### Key Takeaways from the Module:

1. **Metrics for Evaluation:**
   - **Objective Metrics:** Include correctness, speed, and relevancy.
   - **Subjective Metrics:** Involve coherence, tone, and clarity.

2. **Evaluation Techniques:**
   - Surveys, interviews, A/B testing are useful methods for assessment.
   - They help gauge user satisfaction, response relevancy, and engagement.

3. **Parameter Adjustments:**
   - Modifying parameters (like temperature, frequency penalty, presence penalty) influences model output.
   - Token highlighting provides insight into word selection probabilities.

4. **Moving Forward:**
   - Explore advanced prompting techniques in the upcoming module.

This module emphasized the importance of diverse evaluation metrics, techniques for assessment, parameter adjustments, and understanding model behavior through token highlighting. Ready to delve into advanced prompting techniques in the next phase!